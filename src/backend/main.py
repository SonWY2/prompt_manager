import os
import json
import datetime
import re
import aiohttp
from fastapi import FastAPI, HTTPException, Body
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import uuid

from database import PromptManagerDB

# --- Configuration ---
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
DATA_DIR = os.path.join(ROOT_DIR, 'data')

# --- FastAPI App Initialization ---
app = FastAPI()

# CORS ÏÑ§Ï†ï
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Database Initialization ---
db = PromptManagerDB()

# Í∏∞Ï°¥ TinyDB Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÎã§Î©¥ ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò
legacy_db_path = os.path.join(DATA_DIR, 'db.json')
if os.path.exists(legacy_db_path) and os.path.getsize(legacy_db_path) > 0:
    try:
        print("üîÑ Í∏∞Ï°¥ TinyDB Îç∞Ïù¥ÌÑ∞ Î∞úÍ≤¨, SQLiteÎ°ú ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò ÏãúÎèÑ...")
        if db.migrate_from_tinydb(legacy_db_path):
            # ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò ÏÑ±Í≥µ Ïãú Í∏∞Ï°¥ ÌååÏùº Î∞±ÏóÖ
            backup_path = os.path.join(DATA_DIR, f'db_backup_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
            os.rename(legacy_db_path, backup_path)
            print(f"‚úÖ ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò ÏôÑÎ£å, Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞Îäî {backup_path}Ïóê Î∞±ÏóÖÎê®")
        else:
            print("‚ùå ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò Ïã§Ìå®, ÏàòÎèôÏúºÎ°ú Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏ ÌïÑÏöî")
    except Exception as e:
        print(f"‚ùå ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò Ï§ë Ïò§Î•ò: {e}")

# --- Pydantic Models ---
class TaskCreate(BaseModel):
    taskId: str
    name: str

class TaskUpdate(BaseModel):
    name: Optional[str] = None
    isFavorite: Optional[bool] = None
    variables: Optional[Dict[str, Any]] = None

class VersionCreate(BaseModel):
    versionId: str
    content: str
    description: Optional[str] = None
    name: str
    system_prompt: Optional[str] = None
    variables: Optional[Dict[str, Any]] = None

class VersionUpdate(BaseModel):
    content: Optional[str] = None
    name: Optional[str] = None
    description: Optional[str] = None
    system_prompt: Optional[str] = None
    variables: Optional[Dict[str, Any]] = None

class LLMCall(BaseModel):
    taskId: str
    versionId: str
    inputData: Dict[str, str]
    system_prompt: Optional[str] = None
    endpoint: Optional[Dict[str, Any]] = None

class LLMEndpoint(BaseModel):
    id: str = Field(default_factory=lambda: f"llm-ep-{uuid.uuid4()}")
    name: str
    baseUrl: str
    apiKey: Optional[str] = None
    defaultModel: Optional[str] = None
    description: Optional[str] = None
    contextSize: Optional[int] = None
    isDefault: bool = False
    createdAt: str = Field(default_factory=lambda: datetime.datetime.now().isoformat())

class LLMEndpointUpdate(BaseModel):
    name: Optional[str] = None
    baseUrl: Optional[str] = None
    apiKey: Optional[str] = None
    defaultModel: Optional[str] = None
    description: Optional[str] = None
    contextSize: Optional[int] = None

class TestEndpointModelsRequest(BaseModel):
    baseUrl: str
    apiKey: Optional[str] = None

class TestEndpointChatRequest(BaseModel):
    baseUrl: str
    apiKey: Optional[str] = None
    model: str
    message: str

# --- Helper Functions ---
def render_template(template: str, data: dict) -> str:
    print(f"üîß [DEBUG] ÌÖúÌîåÎ¶ø Î†åÎçîÎßÅ ÏãúÏûë:")
    print(f"  - ÏõêÎ≥∏ ÌÖúÌîåÎ¶ø: {template}")
    print(f"  - Î≥ÄÏàò Îç∞Ïù¥ÌÑ∞: {data}")
    
    rendered = template
    for key, value in data.items():
        placeholder = f"{{{{{key}}}}}"
        print(f"  - ÏπòÌôò: {placeholder} -> {str(value)}")
        rendered = rendered.replace(placeholder, str(value))
    
    print(f"  - ÏµúÏ¢Ö Î†åÎçîÎßÅ Í≤∞Í≥º: {rendered}")
    return rendered

def get_settings():
    """Get settings with error handling"""
    try:
        settings = db.get_all_settings()
        if not settings:
            # Default settings
            default_settings = {'activeEndpointId': None, 'defaultEndpointId': None}
            for key, value in default_settings.items():
                db.set_setting(key, str(value) if value else "")
            return default_settings
        
        # Convert string values back to appropriate types
        processed_settings = {}
        for key, value in settings.items():
            if value == "" or value == "None":
                processed_settings[key] = None
            else:
                processed_settings[key] = value
        
        return processed_settings
    except Exception as e:
        print(f"Settings error: {e}")
        return {'activeEndpointId': None, 'defaultEndpointId': None}

# --- API Routes ---

# Health Check
@app.get("/api/health")
def health_check():
    return {"status": "healthy", "database": "sqlite"}

# === Tasks ===
@app.get("/api/tasks")
def get_tasks():
    """Get all tasks with error handling"""
    try:
        tasks_list = db.get_all_tasks()
        return {"tasks": tasks_list}
    except Exception as e:
        print(f"Error getting tasks: {e}")
        return {"tasks": []}

@app.post("/api/tasks", status_code=201)
def create_task(task: TaskCreate):
    try:
        new_task = db.create_task(task.taskId, task.name)
        return {"success": True, "task": new_task}
    except Exception as e:
        print(f"Error creating task: {e}")
        raise HTTPException(status_code=500, detail="Failed to create task")

@app.patch("/api/tasks/{task_id}")
def update_task(task_id: str, updates: TaskUpdate):
    try:
        success = db.update_task(task_id, **updates.dict(exclude_unset=True))
        if not success:
            raise HTTPException(status_code=404, detail="Task not found")
        return {"success": True}
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error updating task: {e}")
        raise HTTPException(status_code=500, detail="Failed to update task")

@app.delete("/api/tasks/{task_id}")
def delete_task(task_id: str):
    try:
        success = db.delete_task(task_id)
        if not success:
            raise HTTPException(status_code=404, detail="Task not found")
        return {"success": True, "message": f"Task {task_id} deleted successfully"}
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error deleting task: {e}")
        raise HTTPException(status_code=500, detail="Failed to delete task")

# === Versions ===
@app.get("/api/tasks/{task_id}/versions")
def get_task_versions(task_id: str):
    try:
        versions = db.get_task_versions(task_id)
        return {"versions": versions}
    except Exception as e:
        print(f"Error getting versions: {e}")
        return {"versions": []}

@app.get("/api/tasks/{task_id}/versions/{version_id}")
def get_version(task_id: str, version_id: str):
    try:
        version = db.get_version_by_id(version_id)
        if not version:
            raise HTTPException(status_code=404, detail="Version not found")
        return version
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error getting version: {e}")
        raise HTTPException(status_code=500, detail="Failed to get version")

@app.post("/api/tasks/{task_id}/versions", status_code=201)
def create_version(task_id: str, version: VersionCreate):
    try:
        new_version = db.create_version(
            version_id=version.versionId,
            task_id=task_id,
            name=version.name,
            content=version.content,
            system_prompt=version.system_prompt or "You are a helpful AI Assistant",
            description=version.description or "",
            variables=version.variables
        )
        return {"success": True, "version": new_version}
    except Exception as e:
        print(f"Error creating version: {e}")
        raise HTTPException(status_code=500, detail="Failed to create version")

@app.put("/api/tasks/{task_id}/versions/{version_id}")
def update_version(task_id: str, version_id: str, updates: VersionUpdate):
    try:
        success = db.update_version(version_id, **updates.dict(exclude_unset=True))
        if not success:
            raise HTTPException(status_code=404, detail="Version not found")
        return {"success": True}
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error updating version: {e}")
        raise HTTPException(status_code=500, detail="Failed to update version")

@app.delete("/api/tasks/{task_id}/versions/{version_id}")
def delete_version(task_id: str, version_id: str):
    try:
        success = db.delete_version(version_id)
        if not success:
            raise HTTPException(status_code=404, detail="Version not found")
        return {"success": True, "message": f"Version {version_id} deleted successfully"}
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error deleting version: {e}")
        raise HTTPException(status_code=500, detail="Failed to delete version")

@app.delete("/api/tasks/{task_id}/versions/{version_id}/results/{timestamp}")
def delete_history_item(task_id: str, version_id: str, timestamp: str):
    try:
        success = db.delete_result(version_id, timestamp)
        if not success:
            raise HTTPException(status_code=404, detail="History item not found")
        return {"success": True, "message": "History item deleted successfully"}
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error deleting history item: {e}")
        raise HTTPException(status_code=500, detail="Failed to delete history item")

# Task Variables Management
@app.get("/api/tasks/{task_id}/variables")
def get_task_variables(task_id: str):
    print(f"üîß [DEBUG] Task Î≥ÄÏàò Î∂àÎü¨Ïò§Í∏∞ ÏöîÏ≤≠: task_id={task_id}")
    
    try:
        task = db.get_task_by_id(task_id)
        if not task:
            print(f"‚ùå [ERROR] Task {task_id} Ï∞æÏùÑ Ïàò ÏóÜÏùå")
            raise HTTPException(status_code=404, detail="Task not found")
        
        variables = task.get("variables", {})
        print(f"üîß [DEBUG] Task {task_id} Î≥ÄÏàò Î∂àÎü¨Ïò§Í∏∞ ÏôÑÎ£å: {variables}")
        
        return {"variables": variables}
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error getting task variables: {e}")
        return {"variables": {}}

@app.put("/api/tasks/{task_id}/variables")
def update_task_variables(task_id: str, request_data: Dict[str, Any]):
    try:
        # Extract variables from request
        if 'variables' in request_data:
            variables = request_data['variables']
        else:
            variables = request_data
        
        print(f"üîß [DEBUG] Task {task_id} Î≥ÄÏàò ÏóÖÎç∞Ïù¥Ìä∏: {variables}")
        
        success = db.update_task(task_id, variables=variables)
        if not success:
            raise HTTPException(status_code=404, detail="Task not found")
        
        print(f"‚úÖ Task {task_id} Î≥ÄÏàò Ï†ÄÏû• ÏôÑÎ£å: {variables}")
        return {"success": True, "variables": variables}
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error updating task variables: {e}")
        raise HTTPException(status_code=500, detail="Failed to update variables")

# Template Variable Management
@app.get("/api/templates/{task_id}/variables")
def get_template_variables(task_id: str):
    try:
        task = db.get_task_by_id(task_id)
        if not task:
            raise HTTPException(status_code=404, detail="Task not found")

        variables = set()
        for version in task.get("versions", []):
            content = version.get("content", "")
            matches = [m[2:-2].strip() for m in re.findall(r"{{.*?}}", content)]
            variables.update(matches)

        return {"variables": list(variables)}
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error getting template variables: {e}")
        return {"variables": []}

# LLM API Integration
@app.post("/api/llm/call")
async def call_llm_endpoint(call: LLMCall):
    try:
        task = db.get_task_by_id(call.taskId)
        if not task:
            raise HTTPException(status_code=404, detail="Task not found")

        version = db.get_version_by_id(call.versionId)
        if not version:
            raise HTTPException(status_code=404, detail="Version not found")

        # Get active endpoint
        settings = get_settings()
        active_endpoint_id = settings.get('activeEndpointId')
        print(f"üîß [DEBUG] ÏÑ§Ï†ïÏóêÏÑú Í∞ÄÏ†∏Ïò® ÌôúÏÑ± endpoint ID: {active_endpoint_id}")
        
        if not active_endpoint_id:
            print(f"‚ùå [ERROR] ÌôúÏÑ± LLM endpointÍ∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏùå")
            raise HTTPException(status_code=400, detail="No active LLM endpoint configured")
        
        active_endpoint = db.get_llm_endpoint_by_id(active_endpoint_id)
        print(f"üîß [DEBUG] DBÏóêÏÑú Ï°∞ÌöåÎêú ÌôúÏÑ± endpoint: {active_endpoint}")
        
        if not active_endpoint:
            print(f"‚ùå [ERROR] ÌôúÏÑ± LLM endpoint {active_endpoint_id}Î•º DBÏóêÏÑú Ï∞æÏùÑ Ïàò ÏóÜÏùå")
            raise HTTPException(status_code=404, detail="Active LLM endpoint not found")

        # Template rendering
        rendered_prompt = render_template(version["content"], call.inputData)
        system_prompt = call.system_prompt or version.get("system_prompt", "You are a helpful assistant.")
        
        print(f"üîß [DEBUG] LLM API Ìò∏Ï∂ú Ï§ÄÎπÑ:")
        print(f"  - Task ID: {call.taskId}")
        print(f"  - Version ID: {call.versionId}")
        print(f"  - System Prompt: {system_prompt}")
        print(f"  - Rendered User Prompt: {rendered_prompt}")
        print(f"  - Active Endpoint: {active_endpoint.get('name')} ({active_endpoint.get('baseUrl')})")
        print(f"  - Active Endpoint Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞: {active_endpoint}")
        
        # Call actual LLM API
        try:
            # DBÏóêÏÑú Ï°∞ÌöåÌïú endpointÎäî Ïù¥ÎØ∏ camelCaseÎ°ú Î≥ÄÌôòÎêòÏñ¥ ÏûàÏúºÎØÄÎ°ú
            # snake_caseÎ°ú Îã§Ïãú Î≥ÄÌôòÌï¥ÏÑú call_actual_llm_apiÏóê Ï†ÑÎã¨
            endpoint_for_api = {
                'base_url': active_endpoint.get('baseUrl'),
                'api_key': active_endpoint.get('apiKey'), 
                'default_model': active_endpoint.get('defaultModel')
            }
            print(f"üîß [DEBUG] API Ìò∏Ï∂úÏö©ÏúºÎ°ú Î≥ÄÌôòÎêú endpoint: {endpoint_for_api}")
            
            result = await call_actual_llm_api(
                endpoint=endpoint_for_api,
                system_prompt=system_prompt,
                user_prompt=rendered_prompt,
                model=active_endpoint.get('defaultModel')
            )
        except Exception as e:
            # LLM API call failed - return error response
            result = {
                "id": "error-response",
                "object": "chat.completion",
                "created": int(datetime.datetime.now().timestamp()),
                "model": active_endpoint.get('default_model', 'unknown'),
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": f"Error calling LLM API: {str(e)}\n\nRendered prompt was: {rendered_prompt}"
                    },
                    "finish_reason": "error"
                }],
                "error": str(e)
            }

        # Save result
        endpoint_info = {
            "id": active_endpoint['id'],
            "name": active_endpoint['name'],
            "model": active_endpoint.get('default_model')
        }
        
        db.add_result(
            version_id=call.versionId,
            input_data=call.inputData,
            output=result,
            endpoint_info=endpoint_info
        )

        return {"result": result}
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error in LLM call: {e}")
        raise HTTPException(status_code=500, detail="Failed to call LLM API")

# Actual LLM API call function
async def call_actual_llm_api(endpoint: dict, system_prompt: str, user_prompt: str, model: str = None):
    """Ïã§Ï†ú LLM API Ìò∏Ï∂ú"""
    
    print(f"üîß [DEBUG] call_actual_llm_api Ìò∏Ï∂ú - Î∞õÏùÄ endpoint Îç∞Ïù¥ÌÑ∞:")
    print(f"  - endpoint: {endpoint}")
    print(f"  - endpoint ÌÉÄÏûÖ: {type(endpoint)}")
    print(f"  - endpoint ÌÇ§Îì§: {list(endpoint.keys()) if endpoint else 'None'}")
    
    base_url = endpoint.get('base_url', '').rstrip('/')
    api_key = endpoint.get('api_key')
    model = model or endpoint.get('default_model', 'gpt-3.5-turbo')
    
    print(f"üîß [DEBUG] Ï∂îÏ∂úÎêú Ï†ïÎ≥¥:")
    print(f"  - base_url: '{base_url}'")
    print(f"  - api_key Ï°¥Ïû¨: {bool(api_key)}")
    print(f"  - model: '{model}'")
    
    if not base_url:
        print(f"‚ùå [ERROR] Base URLÏù¥ ÎπÑÏñ¥ÏûàÏùå!")
        raise Exception("Base URL is required")
    
    # API ÏöîÏ≤≠ Íµ¨ÏÑ±
    headers = {
        'Content-Type': 'application/json'
    }
    
    # API ÌÇ§ Ï≤òÎ¶¨ Î∞è ÎîîÎ≤ÑÍπÖ
    print(f"üîß [DEBUG] API ÌÇ§ Ï≤òÎ¶¨:")
    print(f"  - API ÌÇ§ Ï°¥Ïû¨ Ïó¨Î∂Ä: {bool(api_key)}")
    print(f"  - API ÌÇ§ Í∏∏Ïù¥: {len(api_key) if api_key else 0}")
    if api_key:
        masked_key = api_key[:8] + '*' * (len(api_key) - 12) + api_key[-4:] if len(api_key) > 12 else api_key[:4] + '*' * 4
        print(f"  - API ÌÇ§ (ÎßàÏä§ÌÇπ): {masked_key}")
    print(f"  - Base URL: {base_url}")
    
    if api_key:
        if 'openai.com' in base_url or 'api.together.xyz' in base_url:
            headers['Authorization'] = f'Bearer {api_key}'
            print(f"  - OpenAI/Together ÌòïÏãù Authorization Ìó§Îçî Ï∂îÍ∞Ä")
        elif 'openrouter.ai' in base_url:
            headers['Authorization'] = f'Bearer {api_key}'
            headers['HTTP-Referer'] = 'https://prompt-manager.local'
            headers['X-Title'] = 'Prompt Manager'
            print(f"  - OpenRouter ÌòïÏãù Authorization Ìó§Îçî Ï∂îÍ∞Ä")
        elif 'anthropic.com' in base_url:
            headers['x-api-key'] = api_key
            headers['anthropic-version'] = '2023-06-01'
            print(f"  - Anthropic ÌòïÏãù x-api-key Ìó§Îçî Ï∂îÍ∞Ä")
        else:
            # Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Bearer ÌÜ†ÌÅ∞ ÏÇ¨Ïö©
            headers['Authorization'] = f'Bearer {api_key}'
            print(f"  - Í∏∞Î≥∏ Bearer ÌÜ†ÌÅ∞ ÌòïÏãùÏúºÎ°ú Authorization Ìó§Îçî Ï∂îÍ∞Ä")
    else:
        print(f"  - API ÌÇ§Í∞Ä ÏóÜÏñ¥ÏÑú Ïù∏Ï¶ù Ìó§Îçî Ï∂îÍ∞ÄÌïòÏßÄ ÏïäÏùå")
    
    print(f"üîß [DEBUG] ÏµúÏ¢Ö ÏöîÏ≤≠ Ìó§Îçî: {headers}")
    
    # Anthropic Claude API
    if 'anthropic.com' in base_url:
        combined_content = f"{system_prompt}\n\n{user_prompt}"
        data = {
            'model': model,
            'max_tokens': 4000,
            'messages': [
                {
                    'role': 'user',
                    'content': combined_content
                }
            ]
        }
        url = f"{base_url}/messages"
        print(f"üîß [DEBUG] Anthropic ChatML Î©îÏãúÏßÄ Íµ¨ÏÑ±:")
        print(f"  - Model: {model}")
        print(f"  - URL: {url}")
        print(f"  - Combined Content: {combined_content}")
        print(f"  - Messages: {data['messages']}")
    else:
        # OpenAI Ìò∏Ìôò API (OpenAI, Together, vLLM, Ollama Îì±)
        messages = []
        
        # System promptÍ∞Ä ÎπÑÏñ¥ÏûàÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ÏóêÎßå Ï∂îÍ∞Ä
        if system_prompt and system_prompt.strip():
            messages.append({'role': 'system', 'content': system_prompt.strip()})
            print(f"üîß [DEBUG] System message Ï∂îÍ∞Ä: {system_prompt.strip()}")
        else:
            print(f"‚ö†Ô∏è [WARNING] System promptÍ∞Ä ÎπÑÏñ¥ÏûàÏùå, Î©îÏãúÏßÄÏóê Ìè¨Ìï®ÌïòÏßÄ ÏïäÏùå")
        
        # User promptÎäî ÌïÑÏàòÏù¥ÎØÄÎ°ú ÌôïÏù∏ ÌõÑ Ï∂îÍ∞Ä
        if user_prompt and user_prompt.strip():
            messages.append({'role': 'user', 'content': user_prompt.strip()})
            print(f"üîß [DEBUG] User message Ï∂îÍ∞Ä: {user_prompt.strip()}")
        else:
            print(f"‚ùå [ERROR] User promptÍ∞Ä ÎπÑÏñ¥ÏûàÏùå!")
            raise Exception("User prompt cannot be empty")
        
        data = {
            'model': model,
            'messages': messages,
            'temperature': 0.7,
            'max_tokens': 4000
        }
        url = f"{base_url}/chat/completions"
        print(f"üîß [DEBUG] OpenAI Ìò∏Ìôò ChatML Î©îÏãúÏßÄ Íµ¨ÏÑ±:")
        print(f"  - Model: {model}")
        print(f"  - URL: {url}")
        print(f"  - Messages count: {len(messages)}")
        print(f"  - Full Messages: {messages}")
    
    # Making LLM API call
    print(f"üîß [DEBUG] Ïã§Ï†ú API ÏöîÏ≤≠:")
    print(f"  - URL: {url}")
    print(f"  - Headers: {headers}")
    print(f"  - Data: {data}")
    
    async with aiohttp.ClientSession() as session:
        try:
            async with session.post(
                url, 
                headers=headers, 
                json=data,
                timeout=aiohttp.ClientTimeout(total=60)  # 60Ï¥à ÌÉÄÏûÑÏïÑÏõÉ
            ) as response:
                
                if response.status != 200:
                    error_text = await response.text()
                    print(f"‚ùå [ERROR] LLM API Ìò∏Ï∂ú Ïã§Ìå®: {response.status} - {error_text}")
                    raise Exception(f"API returned {response.status}: {error_text}")
                
                response_data = await response.json()
                print(f"‚úÖ [DEBUG] LLM API ÏùëÎãµ ÏÑ±Í≥µ:")
                print(f"  - Status: {response.status}")
                print(f"  - Response: {response_data}")
                
                # Anthropic ÏùëÎãµÏùÑ OpenAI ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò
                if 'anthropic.com' in base_url:
                    return {
                        "id": response_data.get('id', 'claude-response'),
                        "object": "chat.completion",
                        "created": int(datetime.datetime.now().timestamp()),
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "message": {
                                "role": "assistant",
                                "content": response_data.get('content', [{}])[0].get('text', '')
                            },
                            "finish_reason": response_data.get('stop_reason', 'stop')
                        }],
                        "usage": {
                            "prompt_tokens": response_data.get('usage', {}).get('input_tokens', 0),
                            "completion_tokens": response_data.get('usage', {}).get('output_tokens', 0),
                            "total_tokens": response_data.get('usage', {}).get('input_tokens', 0) + response_data.get('usage', {}).get('output_tokens', 0)
                        }
                    }
                
                return response_data
                
        except aiohttp.ClientTimeout:
            raise Exception("Request timeout")
        except Exception as e:
            raise Exception(f"Network error: {str(e)}")

# === LLM Endpoints ===
@app.get("/api/llm-endpoints")
def get_llm_endpoints():
    try:
        print(f"üîß [DEBUG] LLM Endpoints Ï°∞Ìöå ÏãúÏûë")
        endpoints = db.get_all_llm_endpoints()
        print(f"üîß [DEBUG] DBÏóêÏÑú Ï°∞ÌöåÎêú endpoints: {endpoints}")
        
        settings = get_settings()
        print(f"üîß [DEBUG] ÌòÑÏû¨ settings: {settings}")
        
        response_data = {
            "endpoints": endpoints,
            "activeEndpointId": settings.get('activeEndpointId'),
            "defaultEndpointId": settings.get('defaultEndpointId')
        }
        print(f"‚úÖ [DEBUG] LLM Endpoints ÏùëÎãµ Îç∞Ïù¥ÌÑ∞: {response_data}")
        return response_data
    except Exception as e:
        print(f"‚ùå Error getting LLM endpoints: {e}")
        import traceback
        traceback.print_exc()
        return {"endpoints": [], "activeEndpointId": None, "defaultEndpointId": None}

@app.post("/api/llm-endpoints", status_code=201)
def create_llm_endpoint(endpoint: LLMEndpoint):
    try:
        endpoint_data = endpoint.dict()
        print(f"üîß [DEBUG] LLM Endpoint ÏÉùÏÑ± ÏöîÏ≤≠ Îç∞Ïù¥ÌÑ∞: {endpoint_data}")
        
        # If this is the very first endpoint, make it the default and active one
        existing_endpoints = db.get_all_llm_endpoints()
        print(f"üîß [DEBUG] Í∏∞Ï°¥ ÏóîÎìúÌè¨Ïù∏Ìä∏ Í∞úÏàò: {len(existing_endpoints)}")
        
        if not existing_endpoints:
            endpoint_data["isDefault"] = True
            db.set_setting('activeEndpointId', endpoint_data['id'])
            db.set_setting('defaultEndpointId', endpoint_data['id'])
            print(f"üîß [DEBUG] Ï≤´ Î≤àÏß∏ ÏóîÎìúÌè¨Ïù∏Ìä∏Î°ú ÏÑ§Ï†ï - ID: {endpoint_data['id']}")

        new_endpoint = db.create_llm_endpoint(endpoint_data)
        print(f"‚úÖ [DEBUG] LLM Endpoint ÏÉùÏÑ± ÏôÑÎ£å: {new_endpoint}")
        return {"success": True, "endpoint": new_endpoint}
    except Exception as e:
        print(f"‚ùå Error creating LLM endpoint: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail="Failed to create LLM endpoint")

@app.put("/api/llm-endpoints/{endpoint_id}")
def update_llm_endpoint(endpoint_id: str, updates: LLMEndpointUpdate):
    try:
        update_data = updates.dict(exclude_unset=True)
        print(f"üîß [DEBUG] LLM Endpoint ÏóÖÎç∞Ïù¥Ìä∏ ÏöîÏ≤≠ - ID: {endpoint_id}, Îç∞Ïù¥ÌÑ∞: {update_data}")
        
        success = db.update_llm_endpoint(endpoint_id, **update_data)
        print(f"üîß [DEBUG] Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏóÖÎç∞Ïù¥Ìä∏ Í≤∞Í≥º: {success}")
        
        if not success:
            print(f"‚ùå [DEBUG] LLM Endpoint {endpoint_id} Ï∞æÏùÑ Ïàò ÏóÜÏùå")
            raise HTTPException(status_code=404, detail="LLM endpoint not found")
        
        updated_endpoint = db.get_llm_endpoint_by_id(endpoint_id)
        print(f"‚úÖ [DEBUG] LLM Endpoint ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å: {updated_endpoint}")
        return {"success": True, "endpoint": updated_endpoint}
    except HTTPException:
        raise
    except Exception as e:
        print(f"‚ùå Error updating LLM endpoint: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail="Failed to update LLM endpoint")

@app.delete("/api/llm-endpoints/{endpoint_id}")
def delete_llm_endpoint(endpoint_id: str):
    try:
        success = db.delete_llm_endpoint(endpoint_id)
        if not success:
            raise HTTPException(status_code=404, detail="LLM endpoint not found")
        
        return {"success": True, "message": f"LLM endpoint {endpoint_id} deleted successfully"}
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error deleting LLM endpoint: {e}")
        raise HTTPException(status_code=500, detail="Failed to delete LLM endpoint")

@app.put("/api/llm-endpoints/{endpoint_id}/activate")
def activate_llm_endpoint(endpoint_id: str):
    try:
        endpoint = db.get_llm_endpoint_by_id(endpoint_id)
        if not endpoint:
            raise HTTPException(status_code=404, detail="LLM endpoint not found")
        
        db.set_setting('activeEndpointId', endpoint_id)
        return {"success": True, "message": f"LLM endpoint {endpoint_id} activated successfully"}
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error activating LLM endpoint: {e}")
        raise HTTPException(status_code=500, detail="Failed to activate LLM endpoint")

@app.put("/api/llm-endpoints/{endpoint_id}/set-default")
def set_default_llm_endpoint(endpoint_id: str):
    try:
        endpoint = db.get_llm_endpoint_by_id(endpoint_id)
        if not endpoint:
            raise HTTPException(status_code=404, detail="LLM endpoint not found")
        
        db.set_setting('defaultEndpointId', endpoint_id)
        return {"success": True, "message": f"LLM endpoint {endpoint_id} set as default successfully"}
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error setting default LLM endpoint: {e}")
        raise HTTPException(status_code=500, detail="Failed to set default LLM endpoint")

# === Test Endpoints ===
@app.post("/api/test-endpoint/models")
async def test_models_endpoint(request: TestEndpointModelsRequest):
    """Test the /v1/models endpoint of an LLM provider"""
    try:
        print(f"üîß [DEBUG] Models ÌÖåÏä§Ìä∏ ÏãúÏûë: {request.baseUrl}")
        base_url = request.baseUrl.rstrip('/')
        api_key = request.apiKey
        
        headers = {
            'Content-Type': 'application/json'
        }
        
        # Add API key to headers if provided
        if api_key:
            if 'openai.com' in base_url or 'api.together.xyz' in base_url:
                headers['Authorization'] = f'Bearer {api_key}'
                print(f"üîß [DEBUG] OpenAI/Together Authorization Ìó§Îçî Ï∂îÍ∞Ä")
            elif 'openrouter.ai' in base_url:
                headers['Authorization'] = f'Bearer {api_key}'
                headers['HTTP-Referer'] = 'https://prompt-manager.local'
                headers['X-Title'] = 'Prompt Manager'
                print(f"üîß [DEBUG] OpenRouter Authorization Ìó§Îçî Ï∂îÍ∞Ä")
            elif 'anthropic.com' in base_url:
                headers['x-api-key'] = api_key
                headers['anthropic-version'] = '2023-06-01'
                print(f"üîß [DEBUG] Anthropic x-api-key Ìó§Îçî Ï∂îÍ∞Ä")
            else:
                headers['Authorization'] = f'Bearer {api_key}'
                print(f"üîß [DEBUG] Í∏∞Î≥∏ Bearer ÌÜ†ÌÅ∞ Ìó§Îçî Ï∂îÍ∞Ä")
        
        # Correctly construct the URL
        url = f"{base_url}/models"
        print(f"üîß [DEBUG] ÌÖåÏä§Ìä∏ URL: {url}")
        
        async with aiohttp.ClientSession() as session:
            async with session.get(
                url, 
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                
                if response.status != 200:
                    error_text = await response.text()
                    print(f"‚ùå [DEBUG] Models ÌÖåÏä§Ìä∏ Ïã§Ìå®: {response.status} - {error_text}")
                    raise HTTPException(
                        status_code=response.status, 
                        detail=f"API returned {response.status}: {error_text}"
                    )
                
                response_data = await response.json()
                print(f"‚úÖ [DEBUG] Models ÌÖåÏä§Ìä∏ ÏÑ±Í≥µ")
                return response_data
                
    except aiohttp.ClientError as e:
        print(f"‚ùå [DEBUG] Models ÌÖåÏä§Ìä∏ Ïó∞Í≤∞ Ïò§Î•ò: {e}")
        raise HTTPException(status_code=500, detail=f"Connection error: {str(e)}")
    except Exception as e:
        print(f"‚ùå [DEBUG] Models ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"Test failed: {str(e)}")

@app.post("/api/test-endpoint/chat")
async def test_chat_endpoint(request: TestEndpointChatRequest):
    """Test the /v1/chat/completions endpoint of an LLM provider"""
    try:
        print(f"üîß [DEBUG] Chat ÌÖåÏä§Ìä∏ ÏãúÏûë: {request.baseUrl} - {request.model}")
        print(f"üîß [DEBUG] ÌÖåÏä§Ìä∏ Î©îÏãúÏßÄ: {request.message}")
        
        base_url = request.baseUrl.rstrip('/')
        api_key = request.apiKey
        model = request.model
        message = request.message
        
        headers = {
            'Content-Type': 'application/json'
        }
        
        # Add API key to headers if provided
        if api_key:
            if 'openai.com' in base_url or 'api.together.xyz' in base_url:
                headers['Authorization'] = f'Bearer {api_key}'
                print(f"üîß [DEBUG] OpenAI/Together Authorization Ìó§Îçî Ï∂îÍ∞Ä")
            elif 'openrouter.ai' in base_url:
                headers['Authorization'] = f'Bearer {api_key}'
                headers['HTTP-Referer'] = 'https://prompt-manager.local'
                headers['X-Title'] = 'Prompt Manager'
                print(f"üîß [DEBUG] OpenRouter Authorization Ìó§Îçî Ï∂îÍ∞Ä")
            elif 'anthropic.com' in base_url:
                headers['x-api-key'] = api_key
                headers['anthropic-version'] = '2023-06-01'
                print(f"üîß [DEBUG] Anthropic x-api-key Ìó§Îçî Ï∂îÍ∞Ä")
            else:
                headers['Authorization'] = f'Bearer {api_key}'
                print(f"üîß [DEBUG] Í∏∞Î≥∏ Bearer ÌÜ†ÌÅ∞ Ìó§Îçî Ï∂îÍ∞Ä")
        
        # Prepare request data based on provider
        if 'anthropic.com' in base_url:
            # Anthropic Claude API format
            data = {
                'model': model,
                'max_tokens': 100,
                'messages': [
                    {
                        'role': 'user',
                        'content': message
                    }
                ]
            }
            url = f"{base_url}/messages"
            print(f"üîß [DEBUG] Anthropic ÌòïÌÉú Î©îÏãúÏßÄ Íµ¨ÏÑ±: {data['messages']}")
        else:
            # OpenAI compatible format (Ïò¨Î∞îÎ•∏ ChatML ÌòïÌÉú)
            data = {
                'model': model,
                'messages': [
                    {
                        'role': 'system',
                        'content': 'You are a helpful assistant. Please provide a brief response for testing purposes.'
                    },
                    {
                        'role': 'user',
                        'content': message
                    }
                ],
                'max_tokens': 100,
                'temperature': 0.7
            }
            url = f"{base_url}/chat/completions"
            print(f"üîß [DEBUG] OpenAI Ìò∏Ìôò ChatML ÌòïÌÉú Î©îÏãúÏßÄ Íµ¨ÏÑ±: {data['messages']}")
        
        print(f"üîß [DEBUG] ÌÖåÏä§Ìä∏ URL: {url}")
        print(f"üîß [DEBUG] ÏöîÏ≤≠ Îç∞Ïù¥ÌÑ∞: {data}")
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                url, 
                headers=headers,
                json=data,
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                
                if response.status != 200:
                    error_text = await response.text()
                    print(f"‚ùå [DEBUG] Chat ÌÖåÏä§Ìä∏ Ïã§Ìå®: {response.status} - {error_text}")
                    raise HTTPException(
                        status_code=response.status, 
                        detail=f"API returned {response.status}: {error_text}"
                    )
                
                response_data = await response.json()
                print(f"‚úÖ [DEBUG] Chat ÌÖåÏä§Ìä∏ ÏÑ±Í≥µ: {response_data}")
                
                # Convert Anthropic response to OpenAI format for consistency
                if 'anthropic.com' in base_url:
                    converted_response = {
                        "id": response_data.get('id', 'claude-test'),
                        "object": "chat.completion",
                        "created": int(datetime.datetime.now().timestamp()),
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "message": {
                                "role": "assistant",
                                "content": response_data.get('content', [{}])[0].get('text', 'No response')
                            },
                            "finish_reason": response_data.get('stop_reason', 'stop')
                        }],
                        "usage": {
                            "prompt_tokens": response_data.get('usage', {}).get('input_tokens', 0),
                            "completion_tokens": response_data.get('usage', {}).get('output_tokens', 0),
                            "total_tokens": response_data.get('usage', {}).get('input_tokens', 0) + response_data.get('usage', {}).get('output_tokens', 0)
                        }
                    }
                    print(f"üîß [DEBUG] Anthropic ÏùëÎãµ Î≥ÄÌôò ÏôÑÎ£å: {converted_response}")
                    return converted_response
                
                return response_data
                
    except aiohttp.ClientError as e:
        print(f"‚ùå [DEBUG] Chat ÌÖåÏä§Ìä∏ Ïó∞Í≤∞ Ïò§Î•ò: {e}")
        raise HTTPException(status_code=500, detail=f"Connection error: {str(e)}")
    except Exception as e:
        print(f"‚ùå [DEBUG] Chat ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"Test failed: {str(e)}")

# === Main ===
if __name__ == "__main__":
    import uvicorn
    print("üöÄ SQLite Í∏∞Î∞ò Prompt Manager ÏÑúÎ≤Ñ ÏãúÏûë...")
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)